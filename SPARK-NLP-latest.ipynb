{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMKN0q63t7SPgVXMorM5Zmf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/prateekrana17/SPARK-NLP-RAG-LLM/blob/main/SPARK-NLP-latest.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SxlZ_XaNMVch",
        "outputId": "54d068e9-ef5e-4069-cbdd-22ff031ea399"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-05-15 10:00:55--  https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/scripts/colab_setup.sh\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1191 (1.2K) [text/plain]\n",
            "Saving to: ‘colab_setup.sh.2’\n",
            "\n",
            "\rcolab_setup.sh.2      0%[                    ]       0  --.-KB/s               \rcolab_setup.sh.2    100%[===================>]   1.16K  --.-KB/s    in 0s      \n",
            "\n",
            "2024-05-15 10:00:55 (36.7 MB/s) - ‘colab_setup.sh.2’ saved [1191/1191]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/scripts/colab_setup.sh\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!bash colab_setup.sh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bszxMJupMXVT",
        "outputId": "e20f6672-d15f-4934-f726-c2d42b0959c0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing PySpark 3.2.3 and Spark NLP 5.3.3\n",
            "setup Colab for PySpark 3.2.3 and Spark NLP 5.3.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spark-nlp-display"
      ],
      "metadata": {
        "id": "LFubTgQPoKPr",
        "outputId": "e8e155cd-20e9-4e63-e3e2-6fc97f61133c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spark-nlp-display in /usr/local/lib/python3.10/dist-packages (5.0)\n",
            "Requirement already satisfied: spark-nlp in /usr/local/lib/python3.10/dist-packages (from spark-nlp-display) (5.3.3)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.10/dist-packages (from spark-nlp-display) (7.34.0)\n",
            "Requirement already satisfied: svgwrite==1.4 in /usr/local/lib/python3.10/dist-packages (from spark-nlp-display) (1.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from spark-nlp-display) (2.0.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from spark-nlp-display) (1.25.2)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython->spark-nlp-display) (67.7.2)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.10/dist-packages (from ipython->spark-nlp-display) (0.19.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython->spark-nlp-display) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython->spark-nlp-display) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipython->spark-nlp-display) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython->spark-nlp-display) (3.0.43)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython->spark-nlp-display) (2.16.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython->spark-nlp-display) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython->spark-nlp-display) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython->spark-nlp-display) (4.9.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->spark-nlp-display) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->spark-nlp-display) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->spark-nlp-display) (2024.1)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython->spark-nlp-display) (0.8.4)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython->spark-nlp-display) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->spark-nlp-display) (0.2.13)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->spark-nlp-display) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import sparknlp\n",
        "import pyspark.sql.functions as F\n",
        "\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.sql import SparkSession\n",
        "from sparknlp.annotator import *\n",
        "from sparknlp.base import *\n",
        "from sparknlp.pretrained import PretrainedPipeline\n",
        "from pyspark.sql.types import StringType, IntegerType"
      ],
      "metadata": {
        "id": "UoOE_p5SMW7x"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark = sparknlp.start()\n",
        "spark"
      ],
      "metadata": {
        "id": "m3KahP2uoUn1",
        "outputId": "5f98e42f-7454-4a29-cf0f-df5f2eb8145d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7af5130a7fd0>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://556f9ce42952:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.2.3</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>Spark NLP</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# detection model_name = 'ld_wiki_cnn_231'\n",
        "model_name = 'ld_wiki_tatoeba_cnn_375'"
      ],
      "metadata": {
        "id": "7LiDrLq_MW4n"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"Die Mona Lisa ist ein von Leonardo geschaffenes Ölgemälde aus dem 16. Jahrhundert. Sie findet im Louvre in Paris statt.\"\"\"  #document/ ECM retireved text\n",
        "\n",
        "text2 = \"\"\"Ich liebe Datenwissenschaf\"\"\""
      ],
      "metadata": {
        "id": "BXlMYk7gJSs1"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create spark data frame\n",
        "\n",
        "data = spark.createDataFrame([\n",
        "    [text],\n",
        "    [text2]]).toDF(\"text\")"
      ],
      "metadata": {
        "id": "WmihrZzpJWde"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# detection pipeline for 100+ languages\n",
        "\n",
        "# Step 1: Transforms raw texts to `document` annotation\n",
        "document_assembler = (\n",
        "    DocumentAssembler()\n",
        "    .setInputCol(\"text\")\n",
        "    .setOutputCol(\"document\")\n",
        ")\n",
        "\n",
        "\n",
        "documentAssembler = DocumentAssembler()\\\n",
        "            .setInputCol(\"text\")\\\n",
        "            .setOutputCol(\"document\")\n",
        "\n",
        "sentence_detector = SentenceDetector() \\\n",
        "            .setInputCols([\"document\"]) \\\n",
        "            .setOutputCol(\"sentence\")\n",
        "\n",
        "# Step 2: Determines the language of the text\n",
        "languageDetector = LanguageDetectorDL.pretrained(model_name)\\\n",
        "            .setInputCols(\"sentence\")\\\n",
        "            .setOutputCol(\"language\")\\\n",
        "            .setThreshold(0.5)\\\n",
        "            .setCoalesceSentences(True)\n",
        "\n",
        "nlpPipeline = Pipeline(\n",
        "    stages=[\n",
        "        documentAssembler,\n",
        "        sentence_detector,\n",
        "        languageDetector])\n"
      ],
      "metadata": {
        "id": "MW6ZIZK8mFQC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9fa9d97-432f-4e43-963c-7aff06bd4c3e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ld_wiki_tatoeba_cnn_375 download started this may take some time.\n",
            "Approximate size to download 8.8 MB\n",
            "[OK!]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#language detection\n",
        "result = nlpPipeline.fit(data).transform(data)\n",
        "result.select(\"text\", \"language.result\").show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "da6mS_jBHtZo",
        "outputId": "668d9902-516e-493c-b10c-5dfa7beadd49"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+------+\n",
            "|                text|result|\n",
            "+--------------------+------+\n",
            "|Die Mona Lisa ist...|  [de]|\n",
            "|Ich liebe Datenwi...|  [de]|\n",
            "+--------------------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: fetch the item inside the 'result' column in the dataframe above and store the values in variable d1 and d2\n",
        "\n",
        "language1 = result.select(\"text\", \"language.result\").first()[1]\n",
        "language2 = result.select(\"text\", \"language.result\").take(2)[1][1]\n",
        "\n",
        "print(language1)\n",
        "print(language2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t4U-HETPIdRx",
        "outputId": "75078447-52eb-430e-e4ed-f52f9d334009"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['de']\n",
            "['de']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Translation pipeline\n",
        "documentAssembler = DocumentAssembler()\\\n",
        "  .setInputCol(\"text\")\\\n",
        "  .setOutputCol(\"document\")\n",
        "\n",
        "## More accurate Sentence Detection using Deep Learning\n",
        "sentencerDL = SentenceDetectorDLModel()\\\n",
        "  .pretrained(\"sentence_detector_dl\", \"xx\")\\\n",
        "  .setInputCols([\"document\"])\\\n",
        "  .setOutputCol(\"sentences\")\n",
        "\n",
        "marian = MarianTransformer.pretrained(\"opus_mt_it_en\", \"xx\").setInputCols([\"sentences\"]).setOutputCol(\"translation\")  # add f string variable here to enable translation to language of choice\n",
        "\n",
        "\n",
        "\n",
        "nlp_pipeline = Pipeline(\n",
        "    stages=[\n",
        "        documentAssembler,\n",
        "        sentencerDL,\n",
        "        marian\n",
        "        ])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T5SrBy3EIGuu",
        "outputId": "8fc3a22b-78bb-4a20-f18d-4218f9dd9cc3"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sentence_detector_dl download started this may take some time.\n",
            "Approximate size to download 514.9 KB\n",
            "[OK!]\n",
            "opus_mt_it_en download started this may take some time.\n",
            "Approximate size to download 454.8 MB\n",
            "[OK!]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#open a file and read the text from it into a string variable\n",
        "\n",
        "\n",
        "#code only for colab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "path = '/content/drive/MyDrive/cc/german_example.txt,'  #replace with API call\n",
        "\n",
        "# colab only code ends\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-YiQs9LRIFv4",
        "outputId": "94d1a6ec-a288-43fe-ed3e-7649902841a3"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path = '/content/drive/MyDrive/cc/german_example.txt'  #replace with API call\n",
        "\n",
        "# colab only code ends\n"
      ],
      "metadata": {
        "id": "axTp9LvOu2Ym"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#open a file and read the text from it into a string variable   #replace with API call\n",
        "with open(path, 'r') as file:\n",
        "  print(file.read())\n",
        "  translation_text = file.read()"
      ],
      "metadata": {
        "id": "p8Q4sHiGtZOu",
        "outputId": "97931f51-a9c8-4a5a-c6b2-c419db3425e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        }
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/drive/MyDrive/cc/german_example.txt'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-fa2d9c2c9c85>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#open a file and read the text from it into a string variable   #replace with API call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mtranslation_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/cc/german_example.txt'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#create spark Data Frame for displaying detection results\n",
        "df = spark.createDataFrame(user_text, StringType()).toDF(\"text\")\n",
        "result = nlpPipeline.fit(df).transform(df)"
      ],
      "metadata": {
        "id": "04fFqTanmFMT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tranlation pipeline\n",
        "\n",
        "\n",
        "## More accurate Sentence Detection using Deep Learning\n",
        "sentencerDL = SentenceDetectorDLModel()\\\n",
        "  .pretrained(\"sentence_detector_dl\", \"xx\")\\\n",
        "  .setInputCols([\"document\"])\\\n",
        "  .setOutputCol(\"sentences\")\n",
        "\n",
        "\n",
        "marian = MarianTransformer.pretrained(\"opus_mt_it_en\", \"xx\")\\\n",
        "  .setInputCols([\"sentences\"])\\\n",
        "  .setOutputCol(\"translation\")\n",
        "\n",
        "nlp_pipeline2 = Pipeline(\n",
        "    stages=[\n",
        "        documentAssembler,\n",
        "        sentencerDL,\n",
        "        marian\n",
        "        ])"
      ],
      "metadata": {
        "id": "m7UInfeh8FdH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# translation dataframe\n",
        "\n",
        "empty_df = spark.createDataFrame([[\"\"]]).toDF('text')\n",
        "pipeline_model = nlp_pipeline2.fit(empty_df)\n",
        "lmodel = LightPipeline(pipeline_model)\n",
        "res = lmodel.fullAnnotate(translation_text)\n"
      ],
      "metadata": {
        "id": "ZvsYRe0p9d8O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "M1naHVA49dyZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "V06KFs179dtp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dzC32WLG9dq3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HG8ezZRY9dnN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "p4SxQTXx8FY7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "riLxZTTj7s30"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}